---
title: "Random Cross-Validation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Random Cross-Validation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(sagemaker)
```

You need an aws account and a sagemaker role, with permissions to S3: 
https://stackoverflow.com/a/47767351/6637133

1. Start an sagemaker notebook instance.
   - before you create, you will be prompted to make
     a sagemaker role. Do so




```{r}
xgb_container <- sagemaker$amazon$amazon_estimator$get_image_uri(
  boto3$Session()$region_name,
  "xgboost",
  repo_version = "latest" #version
)
```

```{r}
xgb <- sagemaker$estimator$Estimator(
  xgb_container,
  get_sagemaker_role(),
  train_instance_count = 1L,
  train_instance_type = "ml.m4.xlarge",
  output_path = paste0("s3://", default_bucket(), "/models/"),
  sagemaker_session = sagemaker$Session()
)

xgb
```

```{r}
class(xgb)
```


```{r}
xgb$set_hyperparameters(
  eval_metric = "rmse",
  objective = "reg:linear",
  eta = 0.1,
  gamma = 0.0,
  min_child_weight = 1,
  num_round = 100L,
  early_stopping_rounds = 50L
)
xgb$hyperparam_dict
```

```{r}
hyperparameter_rangers = list(
  max_depth = sagemaker$tuner$IntegerParameter(3L, 20L),
  colsample_bytree = sagemaker$tuner$ContinuousParameter(0, 1),
  subsample = sagemaker$tuner$ContinuousParameter(0, 1)
)

hyperparameter_rangers
```
```{r}
tuning = sagemaker$tuner$HyperparameterTuner(
  xgb,
  "validation:rmse",
  objective_type = "Minimize",
  hyperparameter_rangers,
  strategy = "Random",
  max_jobs = 10L,
  max_parallel_jobs = 2L,
  early_stopping_type = "Auto"
)

tuning
```

```{r}
tuning$static_hyperparameters$eval_metric
```


```{r}
library(rsample)
library(readr)

splits <- initial_split(sagemaker::abalone)

temp_train <- tempfile()
write_csv(training(splits), temp_train, col_names = FALSE)
upload_file(temp_train, key = "abalone-train.csv")

temp_test <- tempfile()
write_csv(testing(splits), temp_test, col_names = FALSE)
upload_file(temp_test, key = "abalone-test.csv")

temp_inference <- tempfile()
sagemaker::abalone %>%
  select(-1) %>%
  write_csv(temp_inference, col_names = FALSE)
upload_file(temp_inference, key = "abalone-inference.csv")
```

```{r}
sagemaker::abalone
```



```{r}
train_data <- sagemaker$s3_input(
  paste0("s3://", default_bucket(), "/abalone-train.csv"),
  content_type = "text/csv"
)

validation_data <- sagemaker$s3_input(
  paste0("s3://", default_bucket(), "/abalone-test.csv"),
  content_type = "text/csv"
)

tuning$fit(
  reticulate::dict(
    train = train_data,
    validation = validation_data
  )
)
```

```{r}
tuning$wait()
```

```{r}
tuning_name <- tuning$latest_tuning_job$job_name
tuning_name
```

```{r}
tuning_parameter_names <- tuning$hyperparameter_ranges() %>%
  discard(is_empty) %>%
  flatten() %>%
  map_chr(pluck, "Name") %>%
  set_names(NULL)
tuning_parameter_names
```


```{r}
tuner <- sagemaker$HyperparameterTuningJobAnalytics(tuning_name)
```




```{r}
library(dplyr)

tuning_stats <- tuner$dataframe()
tuning_stats %>%
  glimpse()

tuning_stats %>%
  janitor::clean_names()
```

```{r}
tuning_stats %>%
  select(FinalObjectiveValue, colsample_bytree:subsample) %>%
  arrange(FinalObjectiveValue)
```

```{r}
best_tuned_model <- tuning_stats %>%
  filter(FinalObjectiveValue == min(FinalObjectiveValue)) %>%
  pull(TrainingJobName)
```

```{r}
tuning_stats %>%
  filter(TrainingJobName == best_tuned_model) %>%
  select(one_of(tuning_parameter_names))
```

```{r}
best_model <- sagemaker$TrainingJobAnalytics(best_tuned_model)
```

```{r}
best_model$dataframe()
```

```{r}
best_model
```

```{r}
test_job <- sagemaker$TrainingJobAnalytics(
  'xgboost-191110-1612-010-70ebc32c'
)
test_job$dataframe()
```


```{r}
sage <- boto3$client('sagemaker')
boto_job <- sage$describe_training_job(
  TrainingJobName = 'xgboost-191110-1612-010-70ebc32c'
)
```


```{r}
# works with job name, don't need full stream name!

cloudwatch <- boto3$client('logs')
log_stream_info <- cloudwatch$describe_log_streams(
  logGroupName = "/aws/sagemaker/TrainingJobs",
  logStreamNamePrefix = "xgboost-191110-1612-010-70ebc32c"
)

log_stream_info$logStreams[[1]]$logStreamName
```

```{r}
job_logs <- system(
  paste0(
    "aws logs get-log-events ",
    "--log-group-name /aws/sagemaker/TrainingJobs ",
    "--log-stream-name xgboost-191110-1612-010-70ebc32c/algo-1-1573424886 ",
    "--output text"
  ),
  intern = TRUE
)
```

```{r}
library(purrr)

metric_names <- boto_job$FinalMetricDataList %>%
  purrr::map_chr(purrr::pluck, "MetricName")

metric_regex <- boto_job$AlgorithmSpecification$MetricDefinitions %>%
  keep(~pluck(., "Name") %in% metric_names) %>%
  map_chr(pluck, "Regex")
```


```{r}
# first is full match, second is string
job_logs %>%
  stringr::str_match(".*\\[[0-9]+\\].*#011train-rmse:([-+]?[0-9]*\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*") %>%
  .[13, ]
```

```{r}
training_logs <- job_logs %>%
  tibble::enframe(NULL, "logs") %>%
  mutate(round = stringr::str_match(logs, ".*\\[([0-9]+)\\].*")[, 2]) %>%
  mutate(round = as.numeric(round)) %>%
  mutate(!!metric_names[1] := stringr::str_match(logs, metric_regex[1])[, 2]) %>%
  mutate(!!metric_names[2] := stringr::str_match(logs, metric_regex[2])[, 2]) %>%
  select(-logs) %>%
  filter_all(all_vars(!is.na(.))) %>%
  arrange(round)

training_logs
```

## Transformers

sagemaker.deploy is a realtime predictor. 
Let's focus on batch for now.

This will be for the `predict` generic.

```{r}
best_tuned_model
```

Transformer doesn't work. 
Need to attach trained estimator and use the transformer method.

Good example to give in justification for package.

```{r}
# prints out logs, need to suppress noise if possible
xf_estimator <- sagemaker$estimator$Estimator$attach(
  training_job_name = best_tuned_model
)
```

```{r}
xfer <- xf_estimator$transformer(
  instance_count = 1L,
  instance_type = "ml.c4.xlarge",
  output_path = paste0("s3://", default_bucket(), "/abalone-transform.csv"),
  assemble_with = 'Line'
)
```

```{r}
xfer
```


```{r}
xfer$transform(
  paste0("s3://", default_bucket(), "/abalone-inference.csv"),
  content_type = "text/csv",
  split_type = "Line",
  wait = TRUE,
  logs = FALSE
)
```

```{r}
xfer$output_path
```


```{r}
download_file(
  "test-inference.csv", "sagemaker-us-east-2-495577990003",
  "abalone-transform.csv/abalone-transform.csv.out"
)
```

```{r}
system(
  paste0(
    "aws s3 cp ", 
    "s3://sagemaker-us-east-2-495577990003/abalone-transform.csv"
  )
)
```

